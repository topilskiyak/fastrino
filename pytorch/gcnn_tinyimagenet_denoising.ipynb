{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTCTqc6L1tnL"
   },
   "source": [
    "# Pytorch Tiny Imagenet GCNN Denoising\n",
    "\n",
    "Resources:\n",
    "- [Tiny Imagenet](https://tiny-imagenet.herokuapp.com/)\n",
    "- [GCNN Paper](https://arxiv.org/abs/1905.12281)\n",
    "- [Building KNN index](https://github.com/rusty1s/pytorch_cluster)\n",
    "\n",
    "Notes:\n",
    "- [unfold](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unfold) / [Fold](https://pytorch.org/docs/stable/nn.html#torch.nn.Fold) / [how-to-use](https://stackoverflow.com/questions/53972159/how-does-pytorchs-fold-and-unfold-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ybkqiEbqsRSe"
   },
   "source": [
    "## Experiment Results\n",
    "\n",
    "- [Experiment logs on comet.ml](https://www.comet.ml/topilskiyak/fastrino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V04rUk1nsUhn"
   },
   "source": [
    "### TLDR\n",
    "\n",
    "Trained a model with the architecture from the [gcnn paper](https://arxiv.org/abs/1905.12281), with 24 hidden features, 8 nearest neighbours, using a batch size of 48, for 10 epochs (**default**).\n",
    "\n",
    "Compared the effectiveness of the graph convolution layers to a model which uses a conv5x5 layer instead of a non-local aggregation with knn (**conv5x5**).\n",
    "\n",
    "The *default* model used way more memory (\\~x10), was way slower (\\~x15) and had lower quality (\\~x3).\n",
    "\n",
    "We fixed this by approximating the non-local aggregation with a dense layer (**approx_aggr**).  \n",
    "The resulting model used way less memory (\\~x6), was a bit faster (\\~x2) and had slightly better quality than *default*. With more tuning it could potentially surpass *conv5x5* in quality.\n",
    "\n",
    "| memory | batch | mse_train | mse_val | link |  \n",
    "| ------ | ----- | --------- | ------- | ---- |  \n",
    "| 9.2GB  | 01.4it/s |  0.029 | 0.034 | [default](https://www.comet.ml/topilskiyak/fastrino/6f32ba37cf94416886e6088dbd3300bc) |  \n",
    "| 1.5GB  | 03.1it/s | - | - | approx_aggr |\n",
    "| 1.1GB  | 21.0it/s | 0.015 | 0.012 | [conv5x5](https://www.comet.ml/topilskiyak/fastrino/9ea5cbfc90384e2c8bb750ca5db53004) |\n",
    "\n",
    "The main hurdle is the large slowdown due to building the knn graph.  \n",
    "\n",
    "A potential solution is to switch from the discrete knn graph to an attention map on all pixels. That way you avoid the computational cost of building a knn graph while still being able to potentially learn it through the attention mapping. Although this approach strays away from the main focus of this current work on exploring gcnns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIZ8cyPZ9wJA"
   },
   "source": [
    "### GCNN Quality: NonLocalAggregationByKNN vs Conv5x5\n",
    "\n",
    "The **default GCNN** setup is:\n",
    "- the architechture from the [gcnn paper](https://arxiv.org/abs/1905.12281)\n",
    "- 24 hidden features\n",
    "- 8 nearest neighbours\n",
    "- 48 batch size\n",
    "\n",
    "Training for 10 epochs on 1/10 of the dataset.  \n",
    "MSE loss on noise (units are $10^{-3}$):  \n",
    "\n",
    "| std | train | test | val | link |\n",
    "| --- | ----- | ---- | --- | ---- |\n",
    "| 0.1 |    14 |   17 |  17 | [std=0.1](https://www.comet.ml/topilskiyak/fastrino/ccde75780048412fb155070ddec8bd7a) |\n",
    "| 0.2 |    29 |   35 |  34 | [std=0.2](https://www.comet.ml/topilskiyak/fastrino/6f32ba37cf94416886e6088dbd3300bc) |\n",
    "| 1.0 |    89 |  108 | 105 | [std=1.0](https://www.comet.ml/topilskiyak/fastrino/ccde75780048412fb155070ddec8bd7a) |\n",
    "\n",
    "Note that the test loss being huge at the start is normal - at that point the model hasn't seen that many training batches yet.\n",
    "\n",
    "Comparison to the case where non-local aggregation is swapped for a simple conv5x5 layer (the **conv5x5** setup).  \n",
    "This is 400 times faster and yields better results, but the best performance given unlimited training time would *probably* be lower.\n",
    "\n",
    "| std | train | test | val | link |\n",
    "| --- | ----- | ---- | --- | ---- |\n",
    "| 0.1 |   4.4 |  5.5 | 5.7 | [std=0.1](https://www.comet.ml/topilskiyak/fastrino/851dd85f73f549829ebce24aeeb95ed6) |\n",
    "| 0.2 |    15 |   12 |  12 | [std=0.2](https://www.comet.ml/topilskiyak/fastrino/9ea5cbfc90384e2c8bb750ca5db53004) |\n",
    "| 1.0 |    68 |   87 |  88 | [std=1.0](https://www.comet.ml/topilskiyak/fastrino/f667de2b49fc44ec85e902da163a66da) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJ7Ps5XK9wJC"
   },
   "source": [
    "### GCNN NonLocalAggregation: GPU Memory Limitations\n",
    "\n",
    "GCNN hits GPU memory limitations pretty hard.\n",
    "\n",
    "The main culprit is probably the intermediate graph convolution aggregation tensor of size BHWKCC:\n",
    "- B - batch\n",
    "- HW - height-width\n",
    "- K - number of nearest neighbours taken\n",
    "- CC - matrix converting hidden features to new hidden features\n",
    "\n",
    "In the original paper: H,W = 32, K = 8, C = 66, HWKCC $\\sim 4 \\cdot 10^7$.  \n",
    "With a batch size of just 16 this results in BHWKCC of $\\sim 0.6 \\cdot 10^9$.  \n",
    "Now consider that each value is float64 (8 bytes), and that these kinds of tensors are used several times in the model ($\\sim$ 10 times). This quickly overwhelms the memory limit of the GPUs use (10-12GB).\n",
    "\n",
    "So we have to balance the size of the hidden features and the batch size.\n",
    "\n",
    "Another problem is that each batch itself trains very slowly, so going throw the whole training/testing/validation datasets takes very long.\n",
    "\n",
    "\n",
    "Results on a Tesla K80:  \n",
    "\n",
    "| hidden | max batch | opt batch | train (1b) | eval (1b) |\n",
    "| ------ | --------- | --------- | ---------- | --------- |\n",
    "|   66   |         8 |         - |       2.0s |      1.5s |\n",
    "|   48   |        16 |         - |       1.8s |      1.2s |\n",
    "|   24   |        56 |        48 |       2.1s |      1.2s |\n",
    "|   12   |       160 |         - |       3.9s |      2.2s |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlnUSXN3yawH"
   },
   "source": [
    "### GCNN NonLocalAggregation: Approximation using a dense layer\n",
    "\n",
    "To overcome GPU memory limitations and for a potential speed-up, we tried to emulate the graph convolution aggregation (to get rid of the BHWKCC tensor).\n",
    "\n",
    "So, we tried using a 1-layer dense NN for the following conversion:\n",
    "$$\n",
    "(F_c - F_{n_1}, ..., F_c - F_{n_k}) \\rightarrow F'_c\n",
    "$$\n",
    "where $F_c$ is the current feature vector, $F_{n_i}$ are the feature vectors of all the neighbours and $F'_c$ is the resulting feature vector.\n",
    "\n",
    "We compared memory consumption and speed for different hidden (h) and batch sizes (b).\n",
    "    \n",
    "| memory | batch | epoch | link | GPU |\n",
    "| ------ | ----- | ----- | ---- | --- |\n",
    "| 9.2GB | 1.4it/s | - | h=24,b=48 (default) | - |\n",
    "| 1.5GB | 3.1it/s | 300s | [h=24,b=48](https://www.comet.ml/topilskiyak/fastrino/67475c227e0e4e2c8cb2a41817cce841) | Tesla T4 |  \n",
    "| 2.6GB | 1.3it/s | 260s | [h=24,b=128](https://www.comet.ml/topilskiyak/fastrino/4477bc26a91a4e19a520dbc019f6ec8a) | Tesla T4 |  \n",
    "| 4.4GB | 0.7it/s | 220s | [h=24,b=256](https://www.comet.ml/topilskiyak/fastrino/e5f0b174a1e74f9693374a1cbef262d1?experiment-tab=systemMetrics) | Tesla T4 |  \n",
    "| 1.4GB | 3.2it/s | 290s | [h=24,b=48](https://www.comet.ml/topilskiyak/fastrino/3ec1b6f40e43437c97511c14b5a52444) | Tesla P100 |  \n",
    "| 2.6GB | 1.0it/s | 900s | [h=66,b=48](https://www.comet.ml/topilskiyak/fastrino/41334b49a5d64222835066de46f38208) | Tesla P100 |  \n",
    "| 5.2GB | 0.38it/s | 910s | [h=66,b=128](https://www.comet.ml/topilskiyak/fastrino/41334b49a5d64222835066de46f38208) | Tesla P100 |  \n",
    "\n",
    "Compared to the default GCNN, the dense approximation uses much less memory and is ~2 times faster.  \n",
    "This allows for larger hidden sizes (for potentially better quality) and larger batch sizes (for potentially faster and more stable training).  \n",
    "\n",
    "Although not observed here, do note that this method potentially trades peak quality for efficiency. Although peak quality of the default setup might be unfeasible due to memory and time constraints.\n",
    "\n",
    "Ideas to try out:\n",
    "- use a 2-layer network\n",
    "- use both the deltas ($F_c - F_{n_i}$) and the neighbours themselves ($F_{n_i}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KR3wqbhQvHym"
   },
   "source": [
    "### GCNN NonLocalAggregation: Speed and Memory impact\n",
    "\n",
    "Using the default setup (h=24, n=8, b=48) as the baseline, we looked into how much the aggregation step impacts speed and memory consumption.\n",
    "\n",
    "| memory | batch | name |\n",
    "| ------ | ----- | ---- |\n",
    "| 9.2GB  | 01.4it/s | default |  \n",
    "| 9.2GB  | 01.4it/s | no bias in aggr_weights |  \n",
    "| 8.6GB  | 01.5it/s | single-layer aggr_weights |  \n",
    "| 8.7GB  | 01.3it/s | single-layer aggr_weights + use einsum |  \n",
    "| 1.5GB  | 03.1it/s | approximation using a single-layer NN |  \n",
    "| 2.0GB | 02.4it/s | conv5x5, but still count aggr_weights (and knn) |  \n",
    "| 1.1GB  | 21.0it/s | conv5x5 (no aggr_weights/knn) |  \n",
    "| 0.9GB  | 35.0it/s | no aggregation or conv5x5 |  \n",
    "| 1.0GB  | 36.0it/s | dcnn |  \n",
    "\n",
    "GPUs used: Tesla P100-PCIE/K80/T4\n",
    "\n",
    "The table shows that:\n",
    "- small changes in how aggregation weights are calculated do not impact speed/memory much  \n",
    "- applying the aggregation weights has the most memory footprint (7GB=9GB-2GB) \n",
    "- approximation of aggregation using a single-layer NN has the best improvement in both speed and memory\n",
    "\n",
    "Ideas to try out:\n",
    "- ECCConv for faster tensor multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXeq5pmGi2wB"
   },
   "source": [
    "### GCNN KNN Graph Construction: Speed impact\n",
    "\n",
    "We'll continue exploring how we can make the model more effecient by looking into how much does it take to make a knn graph.\n",
    "\n",
    "It turns out that the memory impact of KNN Graph construction isn't as large as that of the NonLocalAggregation, so we'll only explore speed in this section.\n",
    "\n",
    "The following table shows that constructing the knn graph causes a 7-fold slowdown (compared to conv5x5):  \n",
    "\n",
    "| speed (it/s) | name |\n",
    "| ------------ | ---- |\n",
    "| 01.37        | default gcnn |\n",
    "| 03.00        | gcnn conv5x5 with knn-graph |\n",
    "| 21.00        | gcnn conv5x5 w/out knn-graph |  \n",
    "\n",
    "The other (2-fold) slowdown is caused by applying the weight aggregation.  \n",
    "This shows that approximation of aggregation using a dense layer is optimal speed-wise.\n",
    "\n",
    "Varying the number of nearest neighbours doesn't seem to impact speed that much:  \n",
    "\n",
    "| speed (it/s) | name |\n",
    "| ------------ | ---- |\n",
    "| 03.00        | gcnn conv5x5 with knn-graph (nn=8) |\n",
    "| 03.30        | gcnn conv5x5 with knn-graph (nn=4) |\n",
    "\n",
    "Note: even though knn_graph uses scipy.stats.cKDTree for CPU knn construction, on GPU it uses a special GPU implementation so that it doesn't have to move tensors from/to GPU.\n",
    "\n",
    "Ideas to try out:\n",
    "- write your own knn using [scipy pykdtree](https://github.com/scipy/scipy/blob/v1.4.1/scipy/spatial/kdtree.py#L185-L942)  \n",
    "- write your own knn using [topk](https://discuss.pytorch.org/t/k-nearest-neighbor-in-pytorch/59695)?\n",
    "- use [faiss](https://github.com/facebookresearch/faiss) (only supports float32)\n",
    "- some more pytorch links:\n",
    " - [approx knn layer](https://discuss.pytorch.org/t/approximate-nearest-neighbors-layer/31466)\n",
    " - [fast pytorch knn](https://discuss.pytorch.org/t/fastest-way-to-find-nearest-neighbor-for-a-set-of-points/5938)\n",
    " - [pytorch loss function](https://discuss.pytorch.org/t/build-your-own-loss-function-in-pytorch/235/2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k75bDitZla99"
   },
   "source": [
    "### General ideas to try out\n",
    "\n",
    "- use gcnn only in some blocks?\n",
    "- [checkpointing](https://pytorch.org/docs/stable/checkpoint.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8XB4rzQrhlm"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3OQ0pKHrjTv"
   },
   "source": [
    "### System Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "colab_type": "code",
    "id": "UizoYlqa4Fkv",
    "outputId": "ee96ae6b-0c1a-4f8a-887f-a172c8fdcf18"
   },
   "outputs": [],
   "source": [
    "# Check that GPU has enough RAM (>10GB)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GclXNKsd9wIp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUGBwYqsJlO2"
   },
   "source": [
    "### Comet_ML\n",
    "\n",
    "NOTE: comet_ml is not installed in google colab by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCTm3b7hoN12"
   },
   "outputs": [],
   "source": [
    "!pip -q install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Id--7c5WJnW1"
   },
   "outputs": [],
   "source": [
    "comet_ml_settings = dict(\n",
    "    api_key=None,\n",
    "    project_name='fastrino',\n",
    "    workspace=None,\n",
    ")\n",
    "\n",
    "assert comet_ml_settings[\"api_key\"] is not None, \"set your comet_ml api_key\"\n",
    "assert comet_ml_settings[\"workspace\"] is not None, \"set your comet_ml workspace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fM-tCnsOoQJS"
   },
   "outputs": [],
   "source": [
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53uX3FGs2JMt"
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtXla-9osExx"
   },
   "outputs": [],
   "source": [
    "# knn_graph\n",
    "!pip -q install torch-cluster\n",
    "# cv2\n",
    "!pip -q install opencv-python\n",
    "# debug\n",
    "!pip -q install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqjArkmIjKA1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch_cluster import knn_graph\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Jo4Q__aSOFZD",
    "outputId": "bd795a57-78fc-4be1-8073-2151c781329e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting torchmemdebug.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile torchmemdebug.py\n",
    "\n",
    "# https://discuss.pytorch.org/t/how-pytorch-releases-variable-garbage/7277/2\n",
    "\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "def mem_report():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpu_stats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RomR9_xxOsW6"
   },
   "outputs": [],
   "source": [
    "import torchmemdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "c6RW5vNqEIXg",
    "outputId": "6fca0390-e886-48f5-8cfc-97d2b1767db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pytorch_ssim.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytorch_ssim.py\n",
    "\n",
    "# https://github.com/Po-Hsun-Su/pytorch-ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNfe3HW_EEKD"
   },
   "outputs": [],
   "source": [
    "import pytorch_ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvvEmsfW2C7q"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rwh2NDcK1KT"
   },
   "source": [
    "Download TinyImagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7h2XduHK48W"
   },
   "outputs": [],
   "source": [
    "if not Path('tiny-imagenet-200.zip').exists():\n",
    "    !wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "    !unzip -q tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRkUzht-KYbE"
   },
   "source": [
    "Full TinyImagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHVSPWQyL835"
   },
   "outputs": [],
   "source": [
    "TINY_IMAGENET_DIR = Path('tiny-imagenet-200')\n",
    "TINY_IMAGENET_PARTS = ['train', 'test', 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HGzyB_pKCUY"
   },
   "outputs": [],
   "source": [
    "def imgdir_to_array(imgdir, take_part=0.1):\n",
    "    images = []\n",
    "    for path in imgdir.iterdir():\n",
    "        img = cv2.imread(str(path))\n",
    "        images.append(img)\n",
    "    stacked = np.stack(images)\n",
    "    take_every = max(1, int(1 / take_part))\n",
    "    taken = stacked[::take_every]\n",
    "    return taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fV8NnJWUIvq_"
   },
   "outputs": [],
   "source": [
    "def dump_cache(source_dict, cache_suffix):\n",
    "    for part, array in source_dict.items():\n",
    "        np.save(part + cache_suffix, array)\n",
    "    print(\"Dumped to cache: \" + cache_suffix)    \n",
    "\n",
    "\n",
    "def load_cache(target_dict, cache_suffix):\n",
    "    for part in TINY_IMAGENET_PARTS:\n",
    "        target_dict[part] = np.load(part + cache_suffix + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLCHwyXTKH4l"
   },
   "outputs": [],
   "source": [
    "class TinyImagenet:\n",
    "    CACHE_SUFFIX = '_full'\n",
    "\n",
    "    def __init__(self, use_cache=True):\n",
    "        self._load_arrays(use_cache)\n",
    "    \n",
    "    def _load_arrays(self, use_cache):\n",
    "        self.arrays = {}\n",
    "        if use_cache:\n",
    "            load_cache(self.arrays, self.CACHE_SUFFIX)\n",
    "            return\n",
    "        \n",
    "        self._load_train()\n",
    "        self._load_test()\n",
    "        self._load_val()\n",
    "\n",
    "        dump_cache(self.arrays, self.CACHE_SUFFIX)\n",
    "    \n",
    "    def _load_train(self):\n",
    "        train_dir = TINY_IMAGENET_DIR / 'train'\n",
    "\n",
    "        image_arrays = []\n",
    "        for imgdir in train_dir.iterdir():\n",
    "            image_arrays.append(imgdir_to_array(imgdir / 'images'))\n",
    "        self.arrays['train'] = np.concatenate(image_arrays, axis=0)\n",
    "\n",
    "    def _load_test(self):\n",
    "        self.arrays['test'] = imgdir_to_array(TINY_IMAGENET_DIR / 'test' / 'images')\n",
    "    \n",
    "    def _load_val(self):\n",
    "        self.arrays['val'] = imgdir_to_array(TINY_IMAGENET_DIR / 'val' / 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DR2pPOjdqfhX"
   },
   "outputs": [],
   "source": [
    "def test_ti(use_cache):\n",
    "    ti = TinyImagenet(use_cache)\n",
    "\n",
    "    for name, arr in ti.arrays.items():\n",
    "        print(name + '\\t', arr.shape)\n",
    "    \n",
    "    for img in [arr[100] for arr in ti.arrays.values()]:\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWp1qG2YrIkH"
   },
   "outputs": [],
   "source": [
    "#test_ti(False)\n",
    "#test_ti(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mC3L0k08-c8P"
   },
   "outputs": [],
   "source": [
    "def print_statistics():\n",
    "    ti = TinyImagenet(use_cache=True)\n",
    "    ars = ti.arrays['train'] / 255\n",
    "    print('mean=', ars.mean(axis=(0,2,3)))\n",
    "    ars -= ars.mean(axis=(0,2,3))[np.newaxis, :, np.newaxis, np.newaxis]\n",
    "    ars **= 2\n",
    "    print('std=', ars.mean(axis=(0,2,3)))\n",
    "\n",
    "#print_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0EXsUjIKh06"
   },
   "source": [
    "TinyImagenet split into patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bwmhd21srS07"
   },
   "outputs": [],
   "source": [
    "def split_into_patches(array, kernel_size=32, stride=32):\n",
    "    _, max_x, max_y, _ = array.shape\n",
    "    patches = []\n",
    "    for x_end in range(kernel_size, max_x + 1, stride):\n",
    "        for y_end in range(kernel_size, max_y + 1, stride):\n",
    "            x_start = x_end - kernel_size\n",
    "            y_start = y_end - kernel_size\n",
    "            patch = array[:, x_start:x_end, y_start:y_end, :]\n",
    "            patches.append(patch)\n",
    "    return np.concatenate(patches, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7usyQuXZKOhO"
   },
   "outputs": [],
   "source": [
    "NORMALIZATION_MEAN = (0.39750364, 0.44806704, 0.48023694)\n",
    "NORMALIZATION_STD  = (0.28158993, 0.26886327, 0.27643643)\n",
    "\n",
    "DEFAULT_TRANSFORMS = \\\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),  # ALSO IMPLICITLY DIVIDES BY 255 AND DOES HWC->CHW\n",
    "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD)\n",
    "    ])\n",
    "\n",
    "\n",
    "class ArrayDataset(Dataset):\n",
    "    def __init__(self, array, transform=None):\n",
    "        if transform is None:\n",
    "            transform = DEFAULT_TRANSFORMS\n",
    "        \n",
    "        self.array = array\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.transform(self.array[index])\n",
    "\n",
    "\n",
    "class TinyImagenetPatches:\n",
    "    CACHE_SUFFIX = '_patches'\n",
    "\n",
    "    def __init__(self, tiny_imagenet=None, initial_transform=None):\n",
    "        self._make_patches(tiny_imagenet)\n",
    "        self._make_datasets(initial_transform)\n",
    "    \n",
    "    def _make_patches(self, tiny_imagenet):\n",
    "        self.patches = {}\n",
    "        if tiny_imagenet is None:\n",
    "            load_cache(self.patches, self.CACHE_SUFFIX)\n",
    "            return\n",
    "        \n",
    "        for name, array in tiny_imagenet.arrays.items():\n",
    "            self.patches[name] = split_into_patches(array)\n",
    "\n",
    "        dump_cache(self.patches, self.CACHE_SUFFIX)\n",
    "    \n",
    "    def _make_datasets(self, transform):\n",
    "        self.datasets = {}\n",
    "        for name, patches in self.patches.items():\n",
    "            self.datasets[name] = ArrayDataset(patches, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kX_eNfWlRWWE"
   },
   "outputs": [],
   "source": [
    "def test_tip(use_cache):\n",
    "    if use_cache:\n",
    "        tip = TinyImagenetPatches()\n",
    "    else:\n",
    "        ti = TinyImagenet(use_cache=True)\n",
    "        tip = TinyImagenetPatches(ti)\n",
    "\n",
    "    for name, arr in tip.patches.items():\n",
    "        print(name + '\\t', arr.shape)\n",
    "    \n",
    "    for img in [arr[100] for arr in tip.patches.values()]:\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WuZ1Mw8quVB-"
   },
   "outputs": [],
   "source": [
    "#test_tip(use_cache=False)\n",
    "#test_tip(use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DiCSNHg9Kkxf"
   },
   "source": [
    "Denoising Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tq0LwjkzrSDE"
   },
   "outputs": [],
   "source": [
    "class DenoisingDataset(Dataset):\n",
    "    def __init__(self, dataset, noise_std):\n",
    "        self.dataset = dataset\n",
    "        self.noise_std = noise_std\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.dataset[index]\n",
    "        noise = self.noise_std * torch.randn_like(image)\n",
    "        noisy_image = image + noise\n",
    "        return noisy_image, noise\n",
    "\n",
    "\n",
    "class TinyImagenetPatchesDenoising:\n",
    "    def __init__(self, noise_std, tiny_imagenet=None):\n",
    "        if tiny_imagenet is None:\n",
    "            tiny_imagenet = TinyImagenetPatches()\n",
    "        self.tiny_imagenet = tiny_imagenet\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        self._make_datasets()\n",
    "\n",
    "    def _make_datasets(self):\n",
    "        self.datasets = {}\n",
    "        for name, dataset in self.tiny_imagenet.datasets.items():\n",
    "            self.datasets[name] = DenoisingDataset(dataset, self.noise_std)\n",
    "    \n",
    "    def get_loaders(self, batch_size):\n",
    "        train = torch.utils.data.DataLoader(\n",
    "            self.datasets['train'], batch_size=batch_size, shuffle=True)\n",
    "        test = torch.utils.data.DataLoader(\n",
    "            self.datasets['test'], batch_size=batch_size, shuffle=False)\n",
    "        val = torch.utils.data.DataLoader(\n",
    "            self.datasets['val'], batch_size=batch_size, shuffle=False)\n",
    "        return train, test, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_mx3xAZ_aGE"
   },
   "outputs": [],
   "source": [
    "def test_tipd():\n",
    "    loaders = TinyImagenetPatchesDenoising(0.1).get_loaders(128)\n",
    "\n",
    "    for name, loader in zip(TINY_IMAGENET_PARTS, loaders):\n",
    "        batch_image, batch_noise = next(iter(loader))\n",
    "        print(name)\n",
    "        print(batch_image.shape, f'mean={batch_image.mean()}', f'std={batch_image.std()}')\n",
    "        print(batch_noise.shape, f'mean={batch_noise.mean()}', f'std={batch_noise.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlakD3ngAocg"
   },
   "outputs": [],
   "source": [
    "#test_tipd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYvbJe4uuGnu"
   },
   "source": [
    "Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NpRGuyXpuJ6b"
   },
   "outputs": [],
   "source": [
    "def cache_exists():\n",
    "    path = Path('.')\n",
    "    for cache_suffix in [TinyImagenet.CACHE_SUFFIX, TinyImagenetPatches.CACHE_SUFFIX]:\n",
    "        for part in TINY_IMAGENET_PARTS:\n",
    "            file_ = path / (part + cache_suffix + '.npy')\n",
    "            if not file_.exists():\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "if not cache_exists():\n",
    "    ti = TinyImagenet(use_cache=False)\n",
    "    TinyImagenetPatches(ti)\n",
    "    assert cache_exists()\n",
    "    os._exit(0)  # Restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPVcnuOa5msh"
   },
   "source": [
    "### DnCNN Model ([Paper](https://arxiv.org/abs/1608.03981))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CLxV4q1BWVE"
   },
   "outputs": [],
   "source": [
    "class DnCNNBlock(nn.Module):\n",
    "    def __init__(self, in_, out):\n",
    "        super(DnCNNBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_, out, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, num_blocks=4, input_image_shape=[3, 32, 32], block_num_filters=64):\n",
    "        super(DnCNN, self).__init__()\n",
    "        \n",
    "        self.name = f'dncnn'\n",
    "        \n",
    "        num_input_channels, *_ = input_image_shape\n",
    "\n",
    "        self.input_convrelu = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, block_num_filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        blocks = [DnCNNBlock(block_num_filters, block_num_filters) for _ in range(num_blocks)]\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "        self.output_conv = nn.Conv2d(block_num_filters, num_input_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_ = self.input_convrelu(x)\n",
    "        blocks = self.blocks(input_)\n",
    "        output = self.output_conv(blocks)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6TGEQtfp2Sa"
   },
   "outputs": [],
   "source": [
    "def get_dncnn_model():\n",
    "    model = DnCNN().to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iu73dPj9tiE3"
   },
   "source": [
    "### GCNN Model ([Paper](https://arxiv.org/abs/1905.12281))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c11wKYwUPhb4"
   },
   "source": [
    "Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLltAdWlSRic"
   },
   "outputs": [],
   "source": [
    "class NonLocalGraph(nn.Module):\n",
    "    def __init__(self, features, near_neigh):\n",
    "        super(NonLocalGraph, self).__init__()\n",
    "        self.features = features\n",
    "        self.k = near_neigh\n",
    "        self._init_neightbours()\n",
    "    \n",
    "    def _init_neightbours(self):\n",
    "        self.neighbours = 1\n",
    "        return\n",
    "\n",
    "        features = self.features.permute((0, 2, 3, 1))\n",
    "        b, h, w, c = features.shape\n",
    "        k = self.k\n",
    "\n",
    "        flat_features = features.reshape((-1, c))\n",
    "\n",
    "        batch_indices = torch.arange(b, device=device)\\\n",
    "                             .unsqueeze(-1).unsqueeze(-1).expand((b, h, w))\\\n",
    "                             .reshape(-1)\n",
    "        \n",
    "        flat_nn = knn_graph(flat_features, batch=batch_indices, k=k, loop=False)\n",
    "        flat_nn = flat_nn[0, :]  # assume that we always find k neighbours\n",
    "        \n",
    "        self.neighbours = flat_nn.reshape((b, h, w, k))\n",
    "    \n",
    "    def get_neighbours(self):\n",
    "        return self.neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jky7UvqR-PMu"
   },
   "outputs": [],
   "source": [
    "class AggregationWeights(nn.Module):\n",
    "    def __init__(self, input_features, output_features, leakyrelu_alpha):\n",
    "        super(AggregationWeights, self).__init__()\n",
    "        self.c_in = input_features\n",
    "        self.c_out = output_features\n",
    "        self.output = nn.Sequential(\n",
    "            #nn.Linear(input_features, input_features),\n",
    "            #nn.LeakyReLU(leakyrelu_alpha),\n",
    "            nn.Linear(input_features, output_features * input_features)\n",
    "        )  # single-layer + no bias = less memory\n",
    "\n",
    "    def forward(self, input_):\n",
    "        b, h, w, k, c_in = input_.shape\n",
    "        c_out = self.c_out\n",
    "\n",
    "        output = self.output(input_)\n",
    "        return output.reshape((b, h, w, k, c_out, c_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BzkpsOIqZQGz"
   },
   "outputs": [],
   "source": [
    "class NonLocalAggregation(nn.Module):\n",
    "    def __init__(self, input_features, output_features, near_neigh, leakyrelu_alpha):\n",
    "        super(NonLocalAggregation, self).__init__()\n",
    "        self.linear = nn.Linear(input_features, output_features)\n",
    "        # self.aggregation_weights = AggregationWeights(input_features, output_features, leakyrelu_alpha)\n",
    "        self.aggregation = nn.Linear(near_neigh * input_features, output_features)\n",
    "        self.activation = nn.LeakyReLU(leakyrelu_alpha)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        input_, non_local_graph = input_\n",
    "        input_ = input_.permute((0, 2, 3, 1))\n",
    "\n",
    "        b, h, w, c = input_.shape\n",
    "        k = non_local_graph.k\n",
    "\n",
    "        indices = non_local_graph.get_neighbours()\n",
    "        indices = indices.reshape(-1).unsqueeze(-1).expand((-1, c))\n",
    "        gathered = torch.gather(input_.reshape((-1, c)), 0, indices)\n",
    "        neighbours = gathered.reshape((b, h, w, k, c))\n",
    "\n",
    "        # aggregation approximation by dense layer\n",
    "        aggregation = self.aggregation(neighbours.view((b, h, w, k * c)))\n",
    "\n",
    "        # default aggregation\n",
    "        # delta = neighbours - input_.unsqueeze(-2)\n",
    "        # weights = self.aggregation_weights(delta)\n",
    "        # aggregation = torch.einsum(\n",
    "        #     \"bhwkqc,bhwkc->bhwq\", weights, neighbours\n",
    "        # ) / k\n",
    "\n",
    "        linear = self.linear(input_)\n",
    "        output = self.activation(aggregation + linear)\n",
    "        return output.permute((0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DxPvyVsoPjTi"
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_features, output_features, near_neigh, leakyrelu_alpha):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "\n",
    "        self.near_neigh = near_neigh\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(input_features, output_features, kernel_size=1, padding=0)\n",
    "        self.conv3x3 = nn.Conv2d(input_features, output_features, kernel_size=3, padding=1)\n",
    "        self.conv5x5 = nn.Conv2d(input_features, output_features, kernel_size=5, padding=2)\n",
    "        #self.non_local_aggregation = NonLocalAggregation(input_features, output_features, near_neigh, leakyrelu_alpha)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        if type(input_) is list:\n",
    "            input_, non_local_graph = input_\n",
    "        else:\n",
    "            non_local_graph = NonLocalGraph(input_, self.near_neigh)\n",
    "\n",
    "        scales = [\n",
    "            self.conv1x1(input_),\n",
    "            self.conv3x3(input_),\n",
    "            self.conv5x5(input_), #self.non_local_aggregation([input_, non_local_graph])\n",
    "        ]\n",
    "        output = torch.mean(torch.stack(scales), dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5RW65EnDRoL5"
   },
   "outputs": [],
   "source": [
    "class GraphConvolutionBlock(nn.Module):\n",
    "    def __init__(self, hidden_features, near_neigh, leakyrelu_alpha):\n",
    "        super(GraphConvolutionBlock, self).__init__()\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            GraphConvolution(hidden_features, hidden_features, near_neigh, leakyrelu_alpha),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.LeakyReLU(leakyrelu_alpha)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        return self.output(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwRIpa_bPa5Z"
   },
   "source": [
    "Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWVbfLGPPg7n"
   },
   "outputs": [],
   "source": [
    "class GCNNResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_features, near_neigh, leakyrelu_alpha):\n",
    "        super(GCNNResidualBlock, self).__init__()\n",
    "\n",
    "        self.near_neigh = near_neigh\n",
    "        num_graphconv_blocks = 3\n",
    "\n",
    "        self.blocks = []\n",
    "        for block_num in range(num_graphconv_blocks):\n",
    "            block = GraphConvolutionBlock(\n",
    "                hidden_features, near_neigh, leakyrelu_alpha)\n",
    "            self.blocks.append(block)\n",
    "            self.add_module(f'block_{block_num}', block)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        non_local_graph = NonLocalGraph(input_, self.near_neigh)\n",
    "\n",
    "        output = input_\n",
    "        for block in self.blocks:\n",
    "            output = block([output, non_local_graph])\n",
    "\n",
    "        return input_ + output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L71dn9DbPVbP"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tT3BSI-0PU15"
   },
   "outputs": [],
   "source": [
    "class GCNNPreprocessingBlockSingleScale(nn.Module):\n",
    "    def __init__(self, kernel_size, input_features, hidden_features, near_neigh, leakyrelu_alpha):\n",
    "        super(GCNNPreprocessingBlockSingleScale, self).__init__()\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(input_features, hidden_features, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.LeakyReLU(leakyrelu_alpha),\n",
    "            GraphConvolutionBlock(hidden_features, near_neigh, leakyrelu_alpha)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        return self.output(input_)\n",
    "\n",
    "\n",
    "class GCNNPreprocessingBlock(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, near_neigh, leakyrelu_alpha, kernel_sizes):\n",
    "        super(GCNNPreprocessingBlock, self).__init__()\n",
    "        hidden_features_scale = hidden_features // len(kernel_sizes)\n",
    "\n",
    "        self.scales = []\n",
    "        for kernel_size in kernel_sizes:\n",
    "            scale = GCNNPreprocessingBlockSingleScale(\n",
    "                kernel_size, input_features, hidden_features_scale, near_neigh, leakyrelu_alpha)\n",
    "            self.scales.append(scale)\n",
    "            self.add_module(f'conv{kernel_size}x{kernel_size}', scale)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        scales = [scale(input_) for scale in self.scales]\n",
    "        output = torch.cat(scales, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YqrXPpF9PRH5"
   },
   "source": [
    "GCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FytdN8zKPSZW"
   },
   "outputs": [],
   "source": [
    "class GCNN(nn.Module):\n",
    "    def __init__(self, input_features=3, hidden_features=66, near_neigh=8, leakyrelu_alpha=1e-2):\n",
    "        super(GCNN, self).__init__()\n",
    "        \n",
    "        self.name = f'gcnn{hidden_features}'\n",
    "        \n",
    "        num_residual_blocks = 2\n",
    "        kernel_sizes = [3, 5, 7]\n",
    "        num_kernels = len(kernel_sizes)\n",
    "        hidden_features = ((hidden_features + num_kernels - 1) // num_kernels) * num_kernels\n",
    "\n",
    "        self.preprocessing = GCNNPreprocessingBlock(\n",
    "            input_features, hidden_features, near_neigh, leakyrelu_alpha, kernel_sizes)\n",
    "\n",
    "        residual_blocks = []\n",
    "        for _ in range(num_residual_blocks):\n",
    "            residual_blocks.append(GCNNResidualBlock(hidden_features, near_neigh, leakyrelu_alpha))\n",
    "        self.residual_blocks = nn.Sequential(*residual_blocks)\n",
    "\n",
    "        self.output_graph_conv = GraphConvolution(hidden_features, input_features, \n",
    "                                                  near_neigh, leakyrelu_alpha)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        preprocess = self.preprocessing(input_)\n",
    "        residual_blocks = self.residual_blocks(preprocess)\n",
    "        output = self.output_graph_conv(residual_blocks)\n",
    "        return input_ + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_Dn_qirtqFj"
   },
   "outputs": [],
   "source": [
    "def get_gcnn_model(hidden_features=66):\n",
    "    model = GCNN(hidden_features=hidden_features).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxNfewzf0oAC"
   },
   "outputs": [],
   "source": [
    "def test_gcnn():\n",
    "    model = get_gcnn_model()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loader, *_ = TinyImagenetPatchesDenoising(noise_std=0.1).get_loaders(batch_size=8)    \n",
    "    image, target = next(iter(loader))\n",
    "    pred = model(image.to(device))\n",
    "\n",
    "    print('loss=', criterion(pred, target.to(device)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOba6ciBQLzA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test_gcnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqMugq5a9wMg"
   },
   "source": [
    "### Load/Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RCi-Ujf9wMi"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = Path('model')\n",
    "\n",
    "def save_model(model, name):\n",
    "    MODEL_DIR.mkdir(exist_ok=True)\n",
    "    torch.save(model.state_dict(), MODEL_DIR / name)\n",
    "\n",
    "def load_model(model, name):\n",
    "    model.load_state_dict(torch.load(MODEL_DIR / name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxoI69A36IaN"
   },
   "source": [
    "### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1EOmJOwzBQZX"
   },
   "outputs": [],
   "source": [
    "def get_criterion():\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion\n",
    "\n",
    "def get_optimizer(model):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrzBCD4n6PW0"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOoXr7CQ0q8A"
   },
   "outputs": [],
   "source": [
    "tiny_imagenet = TinyImagenetPatches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0nCmVawA4vT"
   },
   "outputs": [],
   "source": [
    "def train_step(train_loader, model, criterion, optimizer, experiment, epoch):\n",
    "    with experiment.train():\n",
    "        model.train()\n",
    "\n",
    "        tqdm_notebook_train = tqdm_notebook(\n",
    "            train_loader, desc='train loop', leave=False)\n",
    "\n",
    "        for image, target in tqdm_notebook_train:\n",
    "            pred = model(image.to(device))\n",
    "            loss = criterion(pred, target.to(device))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            experiment.log_metric(\"loss\", loss.item())\n",
    "\n",
    "            \n",
    "def test_step(test_loader, model, criterion, experiment, epoch):\n",
    "    def _step():\n",
    "        model.eval()\n",
    "\n",
    "        mse_losses = []\n",
    "        tqdm_notebook_test = tqdm_notebook(\n",
    "            test_loader, desc='test loop', leave=False)\n",
    "        for image, target in tqdm_notebook_test:\n",
    "            pred = model(image.to(device))\n",
    "            mse_losses.append(criterion(pred, target.to(device)).item())\n",
    "\n",
    "        experiment.log_metric(\"loss\", np.mean(mse_losses), epoch=epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with experiment.test():\n",
    "            _step()\n",
    "\n",
    "            \n",
    "def validate_step(val_loader, model, criterion, experiment):\n",
    "    def _step():\n",
    "        model.eval()\n",
    "\n",
    "        mse_losses = []\n",
    "        tqdm_notebook_val = tqdm_notebook(\n",
    "            val_loader, desc='val loop', leave=False)\n",
    "        for image, target in tqdm_notebook_val:\n",
    "            pred = model(image.to(device))\n",
    "            mse_losses.append(criterion(pred, target.to(device)).item())\n",
    "\n",
    "        experiment.log_metric(\"loss\", np.mean(mse_losses))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        with experiment.validate():\n",
    "            _step()\n",
    "\n",
    "\n",
    "def train(get_model, noise_std=0.1, batch_size=48, num_epochs=10, model_name_prefix=''):\n",
    "    train_loader, test_loader, val_loader = \\\n",
    "        TinyImagenetPatchesDenoising(noise_std, tiny_imagenet).get_loaders(batch_size)\n",
    "\n",
    "    model = get_model()\n",
    "    criterion = get_criterion()\n",
    "    optimizer = get_optimizer(model)\n",
    "\n",
    "    experiment = Experiment(**comet_ml_settings)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(num_epochs), desc='Epoch loop'):\n",
    "        train_step(train_loader, model, criterion, optimizer, experiment, epoch)\n",
    "        test_step(test_loader, model, criterion, experiment, epoch)\n",
    "        save_model(model, f'{model.name}_{int(noise_std * 100)}')\n",
    "        if epoch % 10 == 9:\n",
    "            validate_step(val_loader, model, criterion, experiment)\n",
    "\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-a4N4rcuBRS"
   },
   "outputs": [],
   "source": [
    "train(partial(get_gcnn_model, hidden_features=24),\n",
    "      noise_std=0.2, batch_size=48, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2OssUoh19wM1"
   },
   "source": [
    "### Full inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rteDXUrS9wM3"
   },
   "outputs": [],
   "source": [
    "class InferenceByPatch:\n",
    "    def __init__(self, model, patch_size, batch_size):\n",
    "        self.model = model\n",
    "        self.patch_size = patch_size\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def feed_patches(self, patches):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            patches, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            results = []\n",
    "            for batch in loader:\n",
    "                result =  self.model(batch)\n",
    "                results.append(result)\n",
    "        return torch.cat(results, dim=0)\n",
    "    \n",
    "    def feed_inner(self, input_):\n",
    "        ph, pw = self.patch_size\n",
    "        \n",
    "        insplit = input_.unfold(2, ph, ph).unfold(3, pw, pw)\n",
    "        b, c, nh, nw, ph, pw = insplit.shape\n",
    "        \n",
    "        patches = insplit.permute(0, 2, 3, 1, 4, 5).reshape(-1, c, ph, pw)\n",
    "        results = self.feed_patches(patches)\n",
    "        outsplit = results.reshape(b, nh, nw, c, ph, pw).permute(0, 3, 1, 2, 4, 5)\n",
    "        \n",
    "        output = outsplit.permute(0, 1, 2, 4, 3, 5)\\\n",
    "                         .contiguous().view(b, c, nh * ph, nw * pw)\n",
    "        return output\n",
    "    \n",
    "    def __call__(self, input_):\n",
    "        b, c, h, w = input_.shape\n",
    "        ph, pw = self.patch_size\n",
    "        \n",
    "        output = torch.zeros_like(input_)\n",
    "        overlap_counts = torch.zeros_like(input_)\n",
    "        \n",
    "        top_left = self.feed_inner(input_)\n",
    "        b, c, tlh, tlw = top_left.shape\n",
    "        output[:, :, :tlh, :tlw] += top_left\n",
    "        overlap_counts[:, :, :tlh, :tlw] += 1\n",
    "        \n",
    "        bottom = self.feed_inner(input_[:, :, -ph:, :])\n",
    "        b, c, ph, bw = bottom.shape\n",
    "        output[:, :, -ph:, :bw] += bottom\n",
    "        overlap_counts[:, :, -ph:, :bw] += 1\n",
    "        \n",
    "        right = self.feed_inner(input_[:, :, :, -pw:])\n",
    "        b, c, rh, pw = right.shape\n",
    "        output[:, :, :rh, -pw:] += right\n",
    "        overlap_counts[:, :, :rh, -pw:] += 1\n",
    "        \n",
    "        bottom_right = self.feed_inner(input_[:, :, -ph:, -pw:])\n",
    "        b, c, ph, pw = bottom_right.shape\n",
    "        output[:, :, -ph:, -pw:] += bottom_right\n",
    "        overlap_counts[:, :, -ph:, -pw:] += 1\n",
    "        \n",
    "        output /= overlap_counts\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oN59jbD79wM6"
   },
   "outputs": [],
   "source": [
    "class Id(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Id, self).__init__()\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        for i, (d_t, d_in) in enumerate(zip(self.shape, input_.shape)):\n",
    "            if i == 0:\n",
    "                assert d_in <= d_t  # batch\n",
    "            else:\n",
    "                assert d_in == d_t # c h w\n",
    "        return input_\n",
    "\n",
    "def test_inference(shape_input=(11, 5, 113, 313), shape_model=(3, 5, 7, 9)):\n",
    "    b, c, h, w = shape_model\n",
    "    \n",
    "    infer = InferenceByPatch(Id(shape_model), (h, w), b)\n",
    "    \n",
    "    input_ = torch.randn(shape_input)\n",
    "    output = infer(input_)\n",
    "    assert ((input_ - output) == 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00OsJKHM9wM9"
   },
   "outputs": [],
   "source": [
    "#test_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mp8u1bJzL1R8"
   },
   "outputs": [],
   "source": [
    "NORMALIZATION_MEAN_TENSOR = torch.Tensor(NORMALIZATION_MEAN)[None,:,None,None].to(device)\n",
    "NORMALIZATION_STD_TENSOR  = torch.Tensor(NORMALIZATION_STD)[None,:,None,None].to(device)\n",
    "\n",
    "def denormalize(images):\n",
    "    return images.mul_(NORMALIZATION_STD_TENSOR).add_(NORMALIZATION_MEAN_TENSOR)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "def psnr(pred, target, max_=1.0):\n",
    "    mse = mse_loss(pred, target).item()\n",
    "    return 20 * np.log10(max_) - 10 * np.log10(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CT32b4Cc9wNA"
   },
   "outputs": [],
   "source": [
    "def inference(get_model, load_model_name, \n",
    "              dataset, dataset_name,\n",
    "              return_images=False,\n",
    "              patch_size=(32, 32), model_batch_size=48,\n",
    "              noise_std=0.1, batch_size=128):\n",
    "    model = get_model()\n",
    "    load_model(model, load_model_name)\n",
    "    infer = InferenceByPatch(model, patch_size, model_batch_size)\n",
    "    \n",
    "    denoising_dataset = DenoisingDataset(dataset, noise_std=noise_std)    \n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        denoising_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    criterion = get_criterion()\n",
    "    \n",
    "    losses = []\n",
    "    psnrs = []\n",
    "    ssims = []\n",
    "    images_pred = []\n",
    "    images_target = []\n",
    "\n",
    "    progress_bar = tqdm_notebook(loader, desc='Inference', leave=True)\n",
    "    for img, noise_target in progress_bar:\n",
    "        img = img.to(device)\n",
    "        noise_target = noise_target.to(device)\n",
    "\n",
    "        noise_pred = infer(img)\n",
    "        losses.append(criterion(noise_pred, noise_target).item())\n",
    "\n",
    "        img_pred = denormalize(img - noise_pred)\n",
    "        img_target = denormalize(img - noise_target)\n",
    "        psnrs.append(psnr(img_pred, img_target).item())\n",
    "        ssims.append(pytorch_ssim.ssim(img_pred, img_target).item())\n",
    "\n",
    "        if return_images:\n",
    "            images_pred.append(img_pred.to('cpu').numpy())\n",
    "            images_target.append(img_target.to('cpu').numpy())\n",
    "\n",
    "    print(f'Results for {dataset_name} for {load_model_name}:')\n",
    "    print(f'loss = {np.mean(losses):.5f}')\n",
    "    print(f'psnr = {np.mean(psnrs):.5f}')\n",
    "    print(f'ssim = {np.mean(ssims):.5f}')\n",
    "\n",
    "    if return_images:\n",
    "        return np.concat(images_pred), np.concat(images_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZPZrb-iYm7k"
   },
   "outputs": [],
   "source": [
    "#inference(partial(get_gcnn_model, hidden_features=24),\n",
    "#          load_model_name='gcnn24_20',\n",
    "#          dataset=ArrayDataset(np.load('test' + TinyImagenet.CACHE_SUFFIX + '.npy')), \n",
    "#          dataset_name='test',\n",
    "#          noise_std=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hRGb_ah59wND",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#inference(partial(get_gcnn_model, hidden_features=24),\n",
    "#          load_model_name='gcnn24_100',\n",
    "#          dataset=ArrayDataset(np.load('test' + TinyImagenet.CACHE_SUFFIX + '.npy')), \n",
    "#          dataset_name='test',\n",
    "#          noise_std=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2N5HnokfSA9c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gcnn_tinyimagenet_denoising_june.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
